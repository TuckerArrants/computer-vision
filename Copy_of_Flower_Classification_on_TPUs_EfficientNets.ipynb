{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Copy of Flower Classification on TPUs - EfficientNets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TuckerArrants/flower-classification/blob/master/Copy_of_Flower_Classification_on_TPUs_EfficientNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HdBqeSNynkf",
        "colab_type": "text"
      },
      "source": [
        "# Flower Classification with TPUs\n",
        "\n",
        "**There are many famous computer vision classification problems, like reading handwritten digits or distinguishing between cats and dogs, but all of these are tasks that a regular human can do. While it is impressive that we can build models to do these tasks as well as a human can, it would be more impressive to build a model that can can do something the average person can't do - like look at images of flowers and classify them into 104 different types**\n",
        "\n",
        "**That is the task of this notebook. We have over 16,000 images in our training set and about 7,400 images to classify, which is around 5GB of data in total. I will also be performing some image augmentation, so as you can probably imagine, this will require some extra computational power** \n",
        "\n",
        "**To provide this power, we will use Cloud tensor processings units (TPUs) - hardware accelerators specifically designed for deep learning tasks. Luckily for us, Google Collabs TPUs are free and we do not have any weekly quota - which cannot be said of Kaggle**\n",
        "\n",
        "**Before we begin, special thanks to the starter kernel which helped kickstart this notebook. It can be found [here](https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu). If you enjoy this notebook, please leave an upvote and feel free to comment with any questions/suggestions. Let's begin:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q8osS0kynkf",
        "colab_type": "text"
      },
      "source": [
        "# [Project](http://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ihXyT9Ziynkg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b3521c2-98ed-41a3-8579-2333b7087a94"
      },
      "source": [
        "#the basics\n",
        "from matplotlib import pyplot as plt\n",
        "import math, os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#deep learning basics\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "#get current TensorFlow version fo\n",
        "print(\"Currently using Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently using Tensorflow version 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UjOqAezynkj",
        "colab_type": "text"
      },
      "source": [
        "# I. Configuration\n",
        "\n",
        "**To take advantage of TPUs, we have to do some extra work. For the uninitiated, [this](http://www.tensorflow.org/guide/tpu) is an excellent place to start. We start by checking to see if TensorFlow is using a TPU or not - if it isn't, we set the 'strategy' to its default, which works on CPU and a single GPU, though we will definitely need to use the TPU for the current parameter setups of this notebook (if you use smaller image sizes, you might get away with running on CPU/GPU)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-O9GwYpqynkj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "39e1a16f-f25e-4533-b087-b61f3060e8c9"
      },
      "source": [
        "#detect for TPU - don't need any parameters if TPU_NAME environment variable is set, which it always is on Kaggle\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "\n",
        "#if no TPU is detected, turn set TPU to None\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "#if there is a tpu, connect to it and choose distribution strategy\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "#set to the default TensorFlow strategy if no TPU available\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print(\"# of replicas: \", strategy.num_replicas_in_sync)\n",
        "#if the number of replicas = 8, you are officially using TPUs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  grpc://10.61.182.10:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.61.182.10:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.61.182.10:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG4ZRBl2ynkl",
        "colab_type": "text"
      },
      "source": [
        "**TPUs read data directly from Google Cloud Storage (GCS), so we actually need to copy our dataset to a GCS 'bucket' that is near or 'co-located' with the TPU. I stole the GCS path from a Kaggle kernel that used the `KaggleDataset().get_gcs_path` function, which is unavailble in Collabs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TiXEdF3dynkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import this dataset so we can move it to the TPU\n",
        "GCS_DS_PATH = 'gs://kds-d7e754c065754455c772a9ed9418bb21d44d32f1f94aa1885c92db7a'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thumv7krynkn",
        "colab_type": "text"
      },
      "source": [
        "**To optimize the TPUs bandwith, we cut our dataset into files and then send these files to the different TPU cores. The common format for these files is TFRecords which essentially just takes the pixels of the image and some other information (e.g. a label) and stuffs it into a file. A good number of TFRecord files is 16: so we take our dataset and split it into 16 different TFRecord files and send them to the TPUs**\n",
        "\n",
        "**Note: we need to modify some parameters accordingly because of this, most notably, we need to multiply whatever batch size we intend to use for our model(s) by 16**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "50v2_fYJynko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for reproducibility\n",
        "SEED = 34     #my favorite number\n",
        "\n",
        "#define image size we will use\n",
        "#IMAGE_SIZE = [192, 192]               #if you aren't using TPU\n",
        "IMAGE_SIZE = [512, 512]               #if you are using TPU\n",
        "\n",
        "#how many training samples we want going to TPUs \n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync \n",
        "\n",
        "#how many folds we will use to train our model on\n",
        "FOLDS = 3\n",
        "\n",
        "#list other options we have for image sizes\n",
        "GCS_PATH_SELECT = {\n",
        "    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n",
        "    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n",
        "    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n",
        "    512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n",
        "}\n",
        "\n",
        "#choose 512 image size for best performance\n",
        "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFRBOwuCynkq",
        "colab_type": "text"
      },
      "source": [
        "**For more about getting your data into TFRecord format for TPU processing, see [here](http://www.tensorflow.org/tutorials/load_data/tfrecord)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "luVrIHroynkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels = 3)\n",
        "    #convert image to floats between 0 and 1\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    #size needed for TPU\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "    return image\n",
        "\n",
        "def read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    label = tf.cast(example['class'], tf.int32)\n",
        "    \n",
        "    #returns a dataset of (image, label) pairs\n",
        "    return image, label\n",
        "\n",
        "def read_unlabeled_tfrecord(example):\n",
        "    UNLABELED_TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"id\": tf.io.FixedLenFeature([], tf.string),  # [] means single entry\n",
        "    }\n",
        "    \n",
        "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    idnum = example['id']\n",
        "    #returns a dataset of image(s)\n",
        "    return image, idnum "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiS2VSFtynks",
        "colab_type": "text"
      },
      "source": [
        "**And now we just take the above defined functions and broadcast them across our different datasets. There is also a `data_augment` function and `if_aug` parameters which are to prepare us for when we add image augmentation to our model. For a quick reference on using `tf.image` to perform image augmentation, see [this](http://www.tensorflow.org/tutorials/images/data_augmentation)**\n",
        "\n",
        "**Note: to achieve peak performance, we can use a pipeline that 'prefetches' data for the next step before the current step has finished using `tf.data`. You can learn more [here](http://www.tensorflow.org/guide/data_performance)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FIAgSHfIynks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define pre fetching strategy\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "def load_dataset(filenames, labeled = True, ordered = False):\n",
        "    #order doesn't matter since we will be shuffling the data anyway\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "    \n",
        "    #automatically interleaves reads from multiple files\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "    #use data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    #returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeled = False\n",
        "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO)\n",
        "    return dataset\n",
        "\n",
        "#some simply image augmentation we can perform with tf.image\n",
        "def data_augment(image, label):\n",
        "    \n",
        "    #random augmentations\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    #image = tf.image.random_flip_up_down(image)\n",
        "    #image = tf.image.random_saturation(image, 0, 2)\n",
        "    #image = tf.image.random_hue(image, max_delta = .2)\n",
        "    #image = tf.image.random_brightness(image, max_delta = .2)\n",
        "    \n",
        "    #fixed augmentations\n",
        "    #image = tf.image.adjust_saturation(image, max_delta = .2)\n",
        "    #image = tf.image.central_crop(image, central_fraction = 0.5)\n",
        "    return image, label   \n",
        "\n",
        "def get_training_dataset(dataset,do_aug = True):\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    #the training dataset must repeat for several epochs\n",
        "    dataset = dataset.repeat()\n",
        "    #shuffle the samples to help with overfitting\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    #set batch size\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    #optional augmentation parameter\n",
        "    if do_aug: dataset = dataset.map(mixup_and_cutmix, num_parallel_calls = AUTO)\n",
        "    #prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "def get_validation_dataset(dataset, do_onehot = True):\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    #if we use down below augmentations - cutmix and mixup - we one hot encode\n",
        "    if do_onehot: dataset = dataset.map(onehot, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.cache()\n",
        "    #prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "def get_test_dataset(ordered = False):\n",
        "    dataset = load_dataset(TEST_FILENAMES, labeled = False, ordered = ordered)\n",
        "    #set batch size\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    #prefetch next batch while training (autotune prefetch buffer size)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "def count_data_items(filenames):\n",
        "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "#use tf.io.gfile.glob to find our training and test files from GCS bucket\n",
        "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec') + tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\n",
        "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec')\n",
        "    \n",
        "#show item counts\n",
        "NUM_TRAINING_IMAGES = int( count_data_items(TRAINING_FILENAMES) * (FOLDS-1.)/FOLDS )\n",
        "NUM_VALIDATION_IMAGES = int( count_data_items(TRAINING_FILENAMES) * (1./FOLDS) )\n",
        "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "\n",
        "print('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CanrdQWRynkv",
        "colab_type": "text"
      },
      "source": [
        "# II. Visualization\n",
        "\n",
        "**Now that we have dealt with all the configuring required to use TPUs, we can extract our images from the TPU and finally get a look at our data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2GHkfnNLynkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define flower classes for labeling purposes\n",
        "classes = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n",
        "           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n",
        "           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n",
        "           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n",
        "           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n",
        "           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n",
        "           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n",
        "           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n",
        "           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n",
        "           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n",
        "           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dylMax76ynkx",
        "colab_type": "text"
      },
      "source": [
        "**Define some helper functions to plot our flower images:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F9H9Gnuyynkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#numpy and matplotlib defaults\n",
        "np.set_printoptions(threshold=15, linewidth=80)\n",
        "\n",
        "def batch_to_numpy_images_and_labels(data):\n",
        "    images, labels = data\n",
        "    numpy_images = images.numpy()\n",
        "    numpy_labels = labels.numpy()\n",
        "    #binary strings are image IDs\n",
        "    if numpy_labels.dtype == object:\n",
        "        numpy_labels = [None for _ in enumerate(numpy_images)]\n",
        "    #If no labels, only image IDs, return None for labels (this is the case for test data)\n",
        "    return numpy_images, numpy_labels\n",
        "\n",
        "def title_from_label_and_target(label, correct_label):\n",
        "    if correct_label is None:\n",
        "        return classes[label], True\n",
        "    correct = (label == correct_label)\n",
        "    return \"{} [{}{}{}]\".format(classes[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n",
        "                                classes[correct_label] if not correct else ''), correct\n",
        "\n",
        "def display_one_flower(image, title, subplot, red=False, titlesize=16):\n",
        "    plt.subplot(*subplot)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "    if len(title) > 0:\n",
        "        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
        "    return (subplot[0], subplot[1], subplot[2]+1)\n",
        "    \n",
        "def display_batch_of_images(databatch, predictions=None):\n",
        "    \"\"\"This will work with:\n",
        "    display_batch_of_images(images)\n",
        "    display_batch_of_images(images, predictions)\n",
        "    display_batch_of_images((images, labels))\n",
        "    display_batch_of_images((images, labels), predictions)\n",
        "    \"\"\"\n",
        "    # data\n",
        "    images, labels = batch_to_numpy_images_and_labels(databatch)\n",
        "    if labels is None:\n",
        "        labels = [None for _ in enumerate(images)]\n",
        "        \n",
        "    #auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n",
        "    rows = int(math.sqrt(len(images)))\n",
        "    cols = len(images)//rows\n",
        "        \n",
        "    #size and spacing\n",
        "    FIGSIZE = 13.0\n",
        "    SPACING = 0.1\n",
        "    subplot=(rows,cols,1)\n",
        "    if rows < cols:\n",
        "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
        "    else:\n",
        "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
        "    \n",
        "    #display\n",
        "    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n",
        "        title = '' if label is None else classes[label]\n",
        "        correct = True\n",
        "        if predictions is not None:\n",
        "            title, correct = title_from_label_and_target(predictions[i], label)\n",
        "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n",
        "        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n",
        "    \n",
        "    #get optimal spacing\n",
        "    plt.tight_layout()\n",
        "    if label is None and predictions is None:\n",
        "        plt.subplots_adjust(wspace=0, hspace=0)\n",
        "    else:\n",
        "        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TsdT3Ua7ynkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#first look at training dataset\n",
        "training_dataset = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False)\n",
        "training_dataset = training_dataset.unbatch().batch(20)\n",
        "train_batch = iter(training_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qqlKL-5Vynk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#first look at test dataset\n",
        "test_dataset = get_test_dataset()\n",
        "test_dataset = test_dataset.unbatch().batch(20)\n",
        "test_batch = iter(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3esHKUx1ynk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#view batch of flowers from train\n",
        "display_batch_of_images(next(train_batch))\n",
        "#you can run this cell again and it will load a new batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PD9I9ywnynk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#view batch of flowers from test\n",
        "display_batch_of_images(next(test_batch))\n",
        "#you can run this cell again and it will load a new batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lPM5RSAynk7",
        "colab_type": "text"
      },
      "source": [
        "# III. Augmentation\n",
        "\n",
        "**Deep neural networks are incredibly powerful and often memorize data as opposed to learn data. To prevent overfitting, regularization techniques are implimented, such as dropout and l1/l2 regularization. We could use dropout, but this just removes informative pixels useful for training by blacking out pixels. Surely we can be more creative with image data than this:**\n",
        "\n",
        "**Note: the two following augmentation implementations are taken from Chris Deotte's notebook, which can be found [here](https://www.kaggle.com/cdeotte/cutmix-and-mixup-on-gpu-tpu)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K8967QTynk7",
        "colab_type": "text"
      },
      "source": [
        "### mixup\n",
        "\n",
        "**Now, the augmentation we did above is great, but we are still adding noise to the images which is also leading to information loss. Luckily, we can do better with mixup. Essentially, all mixup does is random converts images to convex combinations of pairs of images and their labels, as seen in the illustration below:**\n",
        "\n",
        "![mixup](http://miro.medium.com/max/362/0*yLCQYAtNAh28LQks.png)\n",
        "Image from [here](http://medium.com/swlh/how-to-do-mixup-training-from-image-files-in-keras-fe1e1c1e6da6)\n",
        "\n",
        "**We can see that we retain information about both images and their labels while introducing regularization into our model. For more on MixUp, read [this](https://arxiv.org/abs/1710.09412)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fBO57Mnsynk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#need to one hot encode images so we can blend their labels like above\n",
        "def onehot(image,label):\n",
        "    CLASSES = len(classes)\n",
        "    return image,tf.one_hot(label,CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "B4qSmJWIynk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mixup(image, label, PROBABILITY = 1.0):\n",
        "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
        "    # output - a batch of images with mixup applied\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    CLASSES = len(classes)\n",
        "    imgs = []; labs = []\n",
        "    for j in range(BATCH_SIZE):\n",
        "        #do mixup with probability PROBABILITY\n",
        "        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.float32)\n",
        "        #chose random number\n",
        "        k = tf.cast( tf.random.uniform([],0,BATCH_SIZE),tf.int32)\n",
        "        a = tf.random.uniform([],0,1)*P \n",
        "        #make mixup image\n",
        "        img1 = image[j,]\n",
        "        img2 = image[k,]\n",
        "        imgs.append((1-a)*img1 + a*img2)\n",
        "        #make cutmix label\n",
        "        if len(label.shape)==1:\n",
        "            lab1 = tf.one_hot(label[j],CLASSES)\n",
        "            lab2 = tf.one_hot(label[k],CLASSES)\n",
        "        else:\n",
        "            lab1 = label[j,]\n",
        "            lab2 = label[k,]\n",
        "        labs.append((1-a)*lab1 + a*lab2)\n",
        "            \n",
        "    #must explicitly reshape so TPU complier knows output shape\n",
        "    image2 = tf.reshape(tf.stack(imgs),(BATCH_SIZE,DIM,DIM,3))\n",
        "    label2 = tf.reshape(tf.stack(labs),(BATCH_SIZE,CLASSES))\n",
        "    return image2,label2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ocO9g5OQynk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#view effects of mixup\n",
        "row = 3; col = 4;\n",
        "row = min(row,BATCH_SIZE//col)\n",
        "all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\n",
        "augmented_element = all_elements.repeat().batch(BATCH_SIZE).map(mixup)\n",
        "\n",
        "for (img,label) in augmented_element:\n",
        "    plt.figure(figsize=(15,int(15*row/col)))\n",
        "    for j in range(row*col):\n",
        "        plt.subplot(row,col,j+1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[j,])\n",
        "    plt.show()\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzdoTaa5ynlB",
        "colab_type": "text"
      },
      "source": [
        "### CutMix\n",
        "\n",
        "**CutMix is essentially the same as mixup except the images are not blended together, rather a random sized block of one image is superimposed on another image. You can read more about it [here](http://arxiv.org/pdf/1905.04899.pdf)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "czXvlt0IynlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cutmix(image, label, PROBABILITY = 1.0):\n",
        "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
        "    # output - a batch of images with cutmix applied\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    CLASSES = len(classes)\n",
        "    \n",
        "    imgs = []; labs = []\n",
        "    for j in range(BATCH_SIZE):\n",
        "        #do cutmix with probability PROBABILITY\n",
        "        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.int32)\n",
        "        #chose random image to cutmix with\n",
        "        k = tf.cast( tf.random.uniform([],0,BATCH_SIZE),tf.int32)\n",
        "        #chose random location\n",
        "        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
        "        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
        "        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n",
        "        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P\n",
        "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
        "        yb = tf.math.minimum(DIM,y+WIDTH//2)\n",
        "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
        "        xb = tf.math.minimum(DIM,x+WIDTH//2)\n",
        "        #make cutmix image\n",
        "        one = image[j,ya:yb,0:xa,:]\n",
        "        two = image[k,ya:yb,xa:xb,:]\n",
        "        three = image[j,ya:yb,xb:DIM,:]\n",
        "        middle = tf.concat([one,two,three],axis=1)\n",
        "        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n",
        "        imgs.append(img)\n",
        "        #make cutmix label\n",
        "        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n",
        "        if len(label.shape)==1:\n",
        "            lab1 = tf.one_hot(label[j],CLASSES)\n",
        "            lab2 = tf.one_hot(label[k],CLASSES)\n",
        "        else:\n",
        "            lab1 = label[j,]\n",
        "            lab2 = label[k,]\n",
        "        labs.append((1-a)*lab1 + a*lab2)\n",
        "            \n",
        "    #must explicitly reshape so TPU complier knows output shape\n",
        "    image2 = tf.reshape(tf.stack(imgs),(BATCH_SIZE,DIM,DIM,3))\n",
        "    label2 = tf.reshape(tf.stack(labs),(BATCH_SIZE,CLASSES))\n",
        "    return image2,label2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hBHX9AoTynlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#view effects of cutmix\n",
        "row = 3; col = 4;\n",
        "row = min(row,BATCH_SIZE//col)\n",
        "all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\n",
        "augmented_element = all_elements.repeat().batch(BATCH_SIZE).map(cutmix)\n",
        "\n",
        "for (img,label) in augmented_element:\n",
        "    plt.figure(figsize=(15,int(15*row/col)))\n",
        "    for j in range(row*col):\n",
        "        plt.subplot(row,col,j+1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[j,])\n",
        "    plt.show()\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_e6V_CrynlF",
        "colab_type": "text"
      },
      "source": [
        "### mixup and CutMix\n",
        "\n",
        "**It is hard to choose which is better, mixup or CutMix. Luckily, we don't actually have to choose because we can just apply both:**\n",
        "\n",
        "**Note: CutMix will occur `SWITCH * CUTMIX_PROB` of the time and mixup will occur `(1 - SWITCH) * MIXUP_PROB` of the time. We will need to experiment a bit to see which convex combination delivers the best performance**\n",
        "\n",
        "**The current setup gives us mixup 33% of the time, CutMix 33% of the time, and no augmentation 33% of the time**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mEL2R5XGynlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create function to apply both cutmix and mixup\n",
        "def mixup_and_cutmix(image,label):\n",
        "    CLASSES = len(classes)\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    #define how often we want to do activate cutmix or mixup\n",
        "    SWITCH = 1/2\n",
        "    #define how often we want cutmix or mixup to activate when switch is active\n",
        "    CUTMIX_PROB = 2/3\n",
        "    MIXUP_PROB = 2/3\n",
        "    #apply cutmix and mixup\n",
        "    image2, label2 = cutmix(image, label, CUTMIX_PROB)\n",
        "    image3, label3 = mixup(image, label, MIXUP_PROB)\n",
        "    imgs = []; labs = []\n",
        "    for j in range(BATCH_SIZE):\n",
        "        P = tf.cast( tf.random.uniform([],0,1)<=SWITCH, tf.float32)\n",
        "        imgs.append(P*image2[j,]+(1-P)*image3[j,])\n",
        "        labs.append(P*label2[j,]+(1-P)*label3[j,])\n",
        "    #must explicitly reshape so TPU complier knows output shape\n",
        "    image4 = tf.reshape(tf.stack(imgs),(BATCH_SIZE,DIM,DIM,3))\n",
        "    label4 = tf.reshape(tf.stack(labs),(BATCH_SIZE,CLASSES))\n",
        "    return image4,label4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xfakA117ynlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#view effects of cutmix/mixup\n",
        "row = 4; col = 4;\n",
        "row = min(row,BATCH_SIZE//col)\n",
        "all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug = False).unbatch()\n",
        "augmented_element = all_elements.repeat().batch(BATCH_SIZE).map(mixup_and_cutmix)\n",
        "\n",
        "for (img,label) in augmented_element:\n",
        "    plt.figure(figsize = (15,int(15*row/col)))\n",
        "    for j in range(row*col):\n",
        "        plt.subplot(row,col,j+1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img[j,])\n",
        "    plt.show()\n",
        "    break\n",
        "    \n",
        "#file_name = 'stylized-image.png'\n",
        "#tensor_to_image(image).save(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXZYTjZjynlJ",
        "colab_type": "text"
      },
      "source": [
        "# IV. Models, Training, and Submission\n",
        "\n",
        "**Now, it will take far too much time for us to train a model ourselves to learn the optimal weights for classifying our flower photos, so we will instead import a model that has already been pre-trained on ImageNet: a large labeled dataset of real-world images**\n",
        "\n",
        "**We will be importing several popular pre-trained models. For a list of all the pre-trained models that can be imported with Keras, see [here](https://keras.io/api/applications/)**\n",
        "\n",
        "**We could just use the pre-trained weights as they are, or we could 'fine-tune' them by cleverly choosing a learning rate schedule for our model's training, but how do we do this?**\n",
        "\n",
        "**Well, we can start with a low learning rate so that our weights don't update too far away from their initial values, and then increase the learning rate as our model trains. Then after allowing our model to explore other weights near its imported weights for a couple epochs, we bring the learning rate back down again to for the rest of training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xhHMPVB_ynlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define epoch parameters\n",
        "EPOCHS = 25                 \n",
        "STEPS_PER_EPOCH = count_data_items(TRAINING_FILENAMES) // BATCH_SIZE\n",
        "\n",
        "#define learning rate parameters\n",
        "LR_START = 0.00001\n",
        "LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n",
        "LR_MIN = 0.00001\n",
        "LR_RAMPUP_EPOCHS = 5\n",
        "LR_SUSTAIN_EPOCHS = 0\n",
        "LR_DECAY = .8\n",
        "\n",
        "#define ramp up and decay\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < LR_RAMPUP_EPOCHS:\n",
        "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
        "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
        "        lr = LR_MAX\n",
        "    else:\n",
        "        lr = (LR_MAX - LR_MIN) * LR_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
        "    return lr\n",
        "    \n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose = True)\n",
        "\n",
        "#visualize learning rate schedule\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y = [lr_schedule(x) for x in rng]\n",
        "plt.plot(rng, y)\n",
        "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubHyxJoyynlK",
        "colab_type": "text"
      },
      "source": [
        "**Now, our final submission must commit in 3 hours or less (see [here](https://www.kaggle.com/docs/tpu)), so we have to find a balance between performance and computation time. I will define a handful of models to fine-tune and we will just have to experiment to see which delivers this perfect balance**\n",
        "\n",
        "**Note(s): for all models below, if we did not use CutMix or mixup, we would not have one hot encoded our labels and hence we would use sparse_categorical metrics instead**\n",
        "\n",
        "**If you decide to use an EfficientNet model for your final model, you need to install something as it is not yet supported by `keras.applications`. There is another weight option for EffNets to consider that outperforms Imagenet weights called 'Noisy Student' that you can read about [here](https://arxiv.org/abs/1911.04252). After much debate, I decided on the EfficientNetB6 as my pre-trained model seeing as we are limited by commit time and the B6 variant performs virtually just as well as the B7 with 20 million fewer parameters. For more on EffNets, read [this](https://arxiv.org/pdf/1905.11946.pdf)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BUTCRJ5GynlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import DenseNet201, Xception, InceptionV3, and InceptionResNetV2\n",
        "from tensorflow.keras.applications import DenseNet201, Xception, InceptionV3, InceptionResNetV2\n",
        "\n",
        "#requirements to use EfficientNet(s)\n",
        "!pip install -q efficientnet\n",
        "import efficientnet.tfkeras as efn\n",
        "\n",
        "#helper function to create our model\n",
        "def get_DenseNet201():\n",
        "    CLASSES = len(classes)\n",
        "    with strategy.scope():\n",
        "        dnet = DenseNet201(\n",
        "            input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
        "            weights = 'imagenet',\n",
        "            include_top = False\n",
        "        )\n",
        "        #make trainable so we can fine-tune\n",
        "        dnet.trainable = True\n",
        "        model = tf.keras.Sequential([\n",
        "            dnet,\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "            #tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(CLASSES, activation = 'softmax',dtype = 'float32')\n",
        "        ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss = 'categorical_crossentropy', \n",
        "        metrics=['categorical_accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "#create Xception model\n",
        "def get_Xception():\n",
        "    CLASSES = len(classes)\n",
        "    with strategy.scope():\n",
        "        xception = Xception(\n",
        "            input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
        "            weights = 'imagenet',\n",
        "            include_top = False\n",
        "        )\n",
        "        #make trainable so we can fine-tune\n",
        "        xception.trainable = True\n",
        "        model = tf.keras.Sequential([\n",
        "            xception,\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "            #tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(CLASSES, activation = 'softmax',dtype = 'float32')\n",
        "        ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss = 'categorical_crossentropy',\n",
        "        metrics=['categorical_accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "#create Inception model\n",
        "def get_InceptionV3():\n",
        "    CLASSES = len(classes)\n",
        "    with strategy.scope():\n",
        "        inception = InceptionV3(\n",
        "            input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
        "            weights = 'imagenet',\n",
        "            include_top = False\n",
        "        )\n",
        "        #make trainable so we can fine-tune\n",
        "        inception.trainable = True\n",
        "        model = tf.keras.Sequential([\n",
        "            inception,\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "            #tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(CLASSES, activation = 'softmax',dtype = 'float32')\n",
        "        ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss = 'categorical_crossentropy',\n",
        "        metrics=['categorical_accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "#create EfficientNetB5 model\n",
        "def get_EfficientNetB5():\n",
        "    CLASSES = len(classes)\n",
        "    with strategy.scope():\n",
        "        efficient = efn.EfficientNetB5(\n",
        "            input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
        "            weights = 'noisy-student', #or imagenet\n",
        "            include_top = False\n",
        "        )\n",
        "        #make trainable so we can fine-tune\n",
        "        efficient.trainable = True\n",
        "        model = tf.keras.Sequential([\n",
        "            efficient,\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "            #tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(CLASSES, activation = 'softmax',dtype = 'float32')\n",
        "        ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss = 'categorical_crossentropy',\n",
        "        metrics=['categorical_accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "#create EfficientNetB6 model\n",
        "def get_EfficientNetB6():\n",
        "    CLASSES = len(classes)\n",
        "    with strategy.scope():\n",
        "        efficient = efn.EfficientNetB6(\n",
        "            input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
        "            weights = 'noisy-student', #or imagenet\n",
        "            include_top = False\n",
        "        )\n",
        "        #make trainable so we can fine-tune\n",
        "        efficient.trainable = True\n",
        "        model = tf.keras.Sequential([\n",
        "            efficient,\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "            #tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(CLASSES, activation = 'softmax',dtype = 'float32')\n",
        "        ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss = 'categorical_crossentropy',\n",
        "        metrics=['categorical_accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "#create EfficientNetB7 model\n",
        "def get_EfficientNetB7():\n",
        "    CLASSES = len(classes)\n",
        "    with strategy.scope():\n",
        "        efficient = efn.EfficientNetB7(\n",
        "            input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
        "            weights = 'noisy-student', #or imagenet\n",
        "            include_top = False\n",
        "        )\n",
        "        #make trainable so we can fine-tune\n",
        "        efficient.trainable = True\n",
        "        model = tf.keras.Sequential([\n",
        "            efficient,\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "            #tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(CLASSES, activation = 'softmax',dtype = 'float32')\n",
        "        ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss = 'categorical_crossentropy',\n",
        "        metrics=['categorical_accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "#create InceptionResNet model\n",
        "def get_InceptionResNetV2():\n",
        "    CLASSES = len(classes)\n",
        "    with strategy.scope():\n",
        "        inception_res = InceptionResNetV2(\n",
        "            input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
        "            weights = 'imagenet',\n",
        "            include_top = False\n",
        "        )\n",
        "        #make trainable so we can fine-tune\n",
        "        inception_res.trainable = True\n",
        "        model = tf.keras.Sequential([\n",
        "            inception_res,\n",
        "            tf.keras.layers.GlobalAveragePooling2D(),\n",
        "            #tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(CLASSES, activation = 'softmax',dtype = 'float32')\n",
        "        ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss = 'categorical_crossentropy',\n",
        "        metrics=['categorical_accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug6K22udynlN",
        "colab_type": "text"
      },
      "source": [
        "**To make our model even more robust, we can also split our data into different folds during training. For example, if `FOLDS = 5`, we would split our data into 5 different folds, train on the first 4 folds, and then validate on the remaining fold. We then repeat this process and cycle through the folds until we have 5 different models that have been trained and validated on 5 different datasets. We then generate predictions with each one of these trained models and take the average of all 5 predictions as our final predictions:**\n",
        "\n",
        "**(Recall that the `FOLDS` parameter was defined much earlier in the notebook as it was needed to determine the size of the validation set)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S4vpuPWuynlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "#train and cross validate in folds\n",
        "def cross_validate(folds = 5):\n",
        "    histories = []\n",
        "    models = []\n",
        "    #early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3)\n",
        "    kfold = KFold(folds, shuffle = True, random_state = SEED)\n",
        "    strat_kfold = StratifiedKFold(folds, shuffle = True, random_state = SEED)\n",
        "    #get data for different folds\n",
        "    for f, (train_index, val_index) in enumerate(kfold.split(TRAINING_FILENAMES)):\n",
        "        print(); print('-'*25)\n",
        "        print('--- FOLD',f+1)\n",
        "        print('-'*25)\n",
        "        train_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[train_index]['TRAINING_FILENAMES']), labeled = True)\n",
        "        val_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[val_index]['TRAINING_FILENAMES']), labeled = True, ordered = True)\n",
        "        print('Datasets loaded')\n",
        "        #train and cross validate\n",
        "        model = get_DenseNet201()\n",
        "        history = model.fit(\n",
        "            get_training_dataset(train_dataset), \n",
        "            steps_per_epoch = STEPS_PER_EPOCH,\n",
        "            epochs = EPOCHS,\n",
        "            callbacks = [lr_callback], #early_stopping\n",
        "            validation_data = get_validation_dataset(val_dataset),\n",
        "            verbose = 2\n",
        "        )\n",
        "        #add different models and histories from each of the folds to list\n",
        "        models.append(model)\n",
        "        histories.append(history)\n",
        "    return histories, models\n",
        "\n",
        "\n",
        "def train_and_predict(folds = 5):\n",
        "    #since we are splitting the dataset and iterating separately on images and ids, order matters.\n",
        "    test_ds = get_test_dataset(ordered = True)\n",
        "    test_images_ds = test_ds.map(lambda image, idnum: image)\n",
        "    print('Start training %i folds'%folds)\n",
        "    histories, models = cross_validate(folds = folds)\n",
        "    print('Computing predictions...')\n",
        "    #get the mean probability of the models from each fold\n",
        "    probabilities = np.average([models[i].predict(test_images_ds) for i in range(folds)], axis = 0)\n",
        "    #round probabilities off\n",
        "    predictions = np.argmax(probabilities, axis=-1)\n",
        "    print('Generating submission.csv file...')\n",
        "    test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n",
        "    #all in one batch\n",
        "    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n",
        "    np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',',\n",
        "               header='id,label', comments='')\n",
        "    return histories, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "12bMAVtZynlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run train and predict\n",
        "histories, models = train_and_predict(folds = FOLDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6PHgKAX-ynlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define function to visualize learning curves\n",
        "def plot_learning_curves(histories): \n",
        "    fig, ax = plt.subplots(1, 2, figsize = (20, 10))\n",
        "    \n",
        "    #plot accuracies\n",
        "    ax[0].plot(histories[0].history['categorical_accuracy'], color = 'C0')\n",
        "    ax[0].plot(histories[0].history['val_categorical_accuracy'], color = 'C1')\n",
        "    ax[0].plot(histories[1].history['categorical_accuracy'], color = 'C0')\n",
        "    ax[0].plot(histories[1].history['val_categorical_accuracy'], color = 'C1')\n",
        "    ax[0].plot(histories[2].history['categorical_accuracy'], color = 'C0')\n",
        "    ax[0].plot(histories[2].history['val_categorical_accuracy'], color = 'C1')\n",
        "    \n",
        "    #plot losses\n",
        "    ax[1].plot(histories[0].history['loss'], color = 'C0')\n",
        "    ax[1].plot(histories[0].history['val_loss'], color = 'C1')\n",
        "    ax[1].plot(histories[1].history['loss'], color = 'C0')\n",
        "    ax[1].plot(histories[1].history['val_loss'], color = 'C1')\n",
        "    ax[1].plot(histories[2].history['loss'], color = 'C0')\n",
        "    ax[1].plot(histories[2].history['val_loss'], color = 'C1')\n",
        "    \n",
        "    #fix legend\n",
        "    ax[0].legend(['train', 'validation'], loc = 'upper left')\n",
        "    ax[1].legend(['train', 'validation'], loc = 'upper right')\n",
        "    \n",
        "    #set master titles\n",
        "    fig.suptitle(\"Model Performance\", fontsize=14)\n",
        "    \n",
        "    #label axis\n",
        "    ax[0].set_ylabel('Accuracy')\n",
        "    ax[0].set_xlabel('Epoch')\n",
        "    ax[1].set_ylabel('Loss')\n",
        "    ax[1].set_xlabel('Epoch')\n",
        "\n",
        "    return plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ingzw3U8ynlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#look at our learning curves to check bias/variance trade off\n",
        "plot_learning_curves(histories)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}